{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from BERT_utility import BERT_utility\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_concepts = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|t=\"(.*)\"'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            concept_dict = dict()\n",
    "            concept_dict['source'] = source\n",
    "            concept_dict['file_name'] = file_name \n",
    "            concept_dict['text'] = pattern_search.group(1)\n",
    "            concept_dict['line_number'] = int(pattern_search.group(2))\n",
    "            concept_dict['begin_word_num'] = int(pattern_search.group(3))\n",
    "            concept_dict['end_word_num'] = int(pattern_search.group(4))\n",
    "            concept_dict['concept_type'] = pattern_search.group(5)\n",
    "            list_of_concepts.append(concept_dict)\n",
    "    \n",
    "    return list_of_concepts\n",
    "\n",
    "def get_assertions_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_assertions = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|t=\".*\"\\|\\|a=\"(.*)\"'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            assertion_dict = dict()\n",
    "            assertion_dict['source'] = source\n",
    "            assertion_dict['file_name'] = file_name \n",
    "            assertion_dict['text'] = pattern_search.group(1)\n",
    "            assertion_dict['line_number'] = int(pattern_search.group(2))\n",
    "            assertion_dict['begin_word_num'] = int(pattern_search.group(3))\n",
    "            assertion_dict['end_word_num'] = int(pattern_search.group(4))\n",
    "            assertion_dict['assertion_type'] = pattern_search.group(5)\n",
    "            list_of_assertions.append(assertion_dict)\n",
    "    \n",
    "    return list_of_assertions\n",
    "\n",
    "def get_relations_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_relations = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|r=\"(.*)\"\\|\\|c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            relation_dict = dict()\n",
    "            relation_dict['source'] = source\n",
    "            relation_dict['file_name'] = file_name \n",
    "            relation_dict['from_text'] = pattern_search.group(1)\n",
    "            relation_dict['from_line_number'] = int(pattern_search.group(2))\n",
    "            relation_dict['from_begin_word_num'] = int(pattern_search.group(3))\n",
    "            relation_dict['from_end_word_num'] = int(pattern_search.group(4))\n",
    "            relation_dict['relation_type'] = pattern_search.group(5)\n",
    "            relation_dict['to_text'] = pattern_search.group(6)\n",
    "            relation_dict['to_line_number'] = int(pattern_search.group(7))\n",
    "            relation_dict['to_begin_word_num'] = int(pattern_search.group(8))\n",
    "            relation_dict['to_end_word_num'] = int(pattern_search.group(9))\n",
    "            list_of_relations.append(relation_dict)\n",
    "    \n",
    "    return list_of_relations\n",
    "\n",
    "def create_pos_dict_concept(concept):\n",
    "    positions = dict()\n",
    "    \n",
    "    for index,row in concept.iterrows():\n",
    "        for i in range(row['begin_word_num'],row['end_word_num']+1):\n",
    "            positions[str(row['line_number'])+\":\"+str(i)] = row['concept_type']\n",
    "    \n",
    "    return positions\n",
    "\n",
    "def create_pos_dict_assertion(assertion):\n",
    "    positions = dict()\n",
    "    \n",
    "    for index,row in assertion.iterrows():\n",
    "        for i in range(row['begin_word_num'],row['end_word_num']+1):\n",
    "            positions[str(row['line_number'])+\":\"+str(i)] = row['assertion_type']\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\concept_assertion_relation_training_data' \n",
    "beth_file_path = data_file_path + r'\\beth'\n",
    "partners_file_path = data_file_path + r'\\partners'\n",
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data'\n",
    "test_data_texts_path =  os.path.dirname(os.getcwd()) + r'\\Data\\test_data'\n",
    "\n",
    "list_of_all_concepts = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\concept'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\concept', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\concept'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\concept', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\concepts'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\concepts', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.DataFrame(list_of_all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>begin_word_num</th>\n",
       "      <th>end_word_num</th>\n",
       "      <th>concept_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>left basilar atelectasis</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>ventral hernia</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>htn</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>spontaneous echo contrast</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>cath</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name                       text  line_number  begin_word_num  \\\n",
       "0   beth  record-105   left basilar atelectasis           55               6   \n",
       "1   beth  record-105             ventral hernia          143               1   \n",
       "2   beth  record-105                        htn           26               0   \n",
       "3   beth  record-105  spontaneous echo contrast           68               1   \n",
       "4   beth  record-105                       cath           21               0   \n",
       "\n",
       "   end_word_num concept_type  \n",
       "0             8      problem  \n",
       "1             2      problem  \n",
       "2             0      problem  \n",
       "3             3      problem  \n",
       "4             0         test  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concept_type\n",
       "problem      19665\n",
       "test         13833\n",
       "treatment    14188\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df.groupby(['concept_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_assertions = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_df = pd.DataFrame(list_of_all_assertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>begin_word_num</th>\n",
       "      <th>end_word_num</th>\n",
       "      <th>assertion_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>left basilar atelectasis</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>ventral hernia</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>htn</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>spontaneous echo contrast</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>80% lm lesion</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name                       text  line_number  begin_word_num  \\\n",
       "0   beth  record-105   left basilar atelectasis           55               6   \n",
       "1   beth  record-105             ventral hernia          143               1   \n",
       "2   beth  record-105                        htn           26               0   \n",
       "3   beth  record-105  spontaneous echo contrast           68               1   \n",
       "4   beth  record-105              80% lm lesion           21               6   \n",
       "\n",
       "   end_word_num assertion_type  \n",
       "0             8        present  \n",
       "1             2        present  \n",
       "2             0        present  \n",
       "3             3         absent  \n",
       "4             8        present  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assertion_type\n",
       "absent                           4190\n",
       "associated_with_someone_else      220\n",
       "conditional                       221\n",
       "hypothetical                      827\n",
       "possible                          961\n",
       "present                         13246\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertion_df.groupby(['assertion_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conditional',\n",
       " 'present',\n",
       " 'possible',\n",
       " 'associated_with_someone_else',\n",
       " 'hypothetical',\n",
       " 'absent']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(list(assertion_df['assertion_type'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_relations = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_df = pd.DataFrame(list_of_all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>from_text</th>\n",
       "      <th>from_line_number</th>\n",
       "      <th>from_begin_word_num</th>\n",
       "      <th>from_end_word_num</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>to_text</th>\n",
       "      <th>to_line_number</th>\n",
       "      <th>to_begin_word_num</th>\n",
       "      <th>to_end_word_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>cath</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TeRP</td>\n",
       "      <td>80% lm lesion</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>pefusion imaging</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>TeRP</td>\n",
       "      <td>perfusion defects</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>drugs</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>TrCP</td>\n",
       "      <td>known allergies</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>metal plate</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>TrAP</td>\n",
       "      <td>gsw</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>creams</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>TrNAP</td>\n",
       "      <td>any incisions</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name         from_text  from_line_number  from_begin_word_num  \\\n",
       "0   beth  record-105              cath                21                    0   \n",
       "1   beth  record-105  pefusion imaging                19                    6   \n",
       "2   beth  record-105             drugs                12                    8   \n",
       "3   beth  record-105       metal plate                26                    7   \n",
       "4   beth  record-105            creams               145                   14   \n",
       "\n",
       "   from_end_word_num relation_type            to_text  to_line_number  \\\n",
       "0                  0          TeRP      80% lm lesion              21   \n",
       "1                  7          TeRP  perfusion defects              19   \n",
       "2                  8          TrCP    known allergies              12   \n",
       "3                  8          TrAP                gsw              26   \n",
       "4                 14         TrNAP      any incisions             145   \n",
       "\n",
       "   to_begin_word_num  to_end_word_num  \n",
       "0                  6                8  \n",
       "1                 12               13  \n",
       "2                  5                6  \n",
       "3                 11               11  \n",
       "4                 20               21  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relation_type\n",
       "PIP      2203\n",
       "TeCP      504\n",
       "TeRP     3053\n",
       "TrAP     2617\n",
       "TrCP      526\n",
       "TrIP      203\n",
       "TrNAP     174\n",
       "TrWP      133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_df.groupby(['relation_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_map = dict()\n",
    "\n",
    "#Treatment improves medical problem\n",
    "sentence_map['TrIP'] = [\"{0} improves {1}.\", \"{1} can be treated by {0}.\"]\n",
    "\n",
    "#Treatment worsens medical problem\n",
    "sentence_map['TrWP'] = [\"{0} worsens {1}.\", \"{0} does not improve {1}.\", \"{0} does not cure {1}.\"]\n",
    "\n",
    "#Treatment causes medical problem\n",
    "sentence_map['TrCP'] = [\"{0} causes {1}.\", \"{0} results in {1}.\", \"{1} is a result of {0}.\"]\n",
    "\n",
    "#Treatment is administered for the medical problem\n",
    "sentence_map['TrAP'] = [\"{0} is prescribed for {1}.\", \"{0} is administered for {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['TrNAP'] = [\"{0} can not be prescribed due to {1}.\", \"{0} is not administered due to {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['TrNRP'] = [\"{0} and {1} has no relation.\"]\n",
    "\n",
    "#Test reveals medical problem\n",
    "sentence_map['TeRP'] = [\"{0} reveals {1}.\", \"{0} indicates {1}.\"]\n",
    "\n",
    "#Test conducted to investigate medical problem\n",
    "sentence_map['TeCP'] = [\"{0} is conducted to check {1}.\", \"{0} is performed to investigate {1}.\"]\n",
    "\n",
    "#Test and problem has no relation\n",
    "sentence_map['TeNRP'] = [\"{0} and {1} has no relation.\"]\n",
    "\n",
    "#Medical problem indicates medical problem\n",
    "sentence_map['PIP'] = [\"{1} can cause {0}.\", \"{0} is a result of {1}\"]\n",
    "\n",
    "#Medical problem indicates medical problem\n",
    "sentence_map['PNP'] = [\"{0} and {1} has no relation.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-4a71114b1884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_problems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mall_other_entities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcept_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'line_number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'line_number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m&\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'begin_word_num'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'begin_word_num'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentity_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mentity_row\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_other_entities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0movalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[0munfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m         \u001b[0mfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munfilled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfinalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[0mfill_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m     \u001b[0mfill_bool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   4343\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4344\u001b[0m             \u001b[0mdowncast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4345\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4346\u001b[0m         )\n\u001b[0;32m   4347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[0;32m   6256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6257\u001b[0m                 new_data = self._data.fillna(\n\u001b[1;32m-> 6258\u001b[1;33m                     \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6259\u001b[0m                 )\n\u001b[0;32m   6260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fillna\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py\u001b[0m in \u001b[0;36misna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\"\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_isna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py\u001b[0m in \u001b[0;36m_isna_new\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    145\u001b[0m         ),\n\u001b[0;32m    146\u001b[0m     ):\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_isna_ndarraylike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCGeneric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py\u001b[0m in \u001b[0;36m_isna_ndarraylike\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;31m# box\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nsp_utility = BERT_utility()\n",
    "\n",
    "relation_encoding_list = list()\n",
    "relation_label_list = list()\n",
    "\n",
    "no_rel_count = 0\n",
    "rel_count = 0\n",
    "exist_rec_list = list()\n",
    "\n",
    "all_relations_dict = {\"treatment\":['TrIP','TrWP','TrCP','TrAP',\"TrNRP\"], \"test\":['TeRP','TeCP','TeNRP'], \"problem\":['PIP','PNP']}\n",
    "no_relations_dict = {\"treatment\":\"TrNRP\", \"test\":\"TeNRP\", \"problem\":\"PNP\"}\n",
    "\n",
    "all_problems = concept_df[concept_df['concept_type']=='problem']\n",
    "\n",
    "for index,row in all_problems.iterrows():\n",
    "    all_other_entities = concept_df[(concept_df['file_name']==row['file_name'])&(concept_df['source']==row['source'])&(concept_df['line_number']==row['line_number'])&(concept_df['begin_word_num']!=row['begin_word_num'])]\n",
    "    \n",
    "    for entity_index,entity_row in all_other_entities.iterrows():\n",
    "        \n",
    "        if(key_to_check in exist_rec_list):\n",
    "            continue\n",
    "        else:\n",
    "            exist_rec_list.append(row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(row[\"begin_word_num\"])+\"#\"+str(entity_row[\"begin_word_num\"]))\n",
    "        \n",
    "        relation_df_record = relation_df[(relation_df['file_name']==row['file_name'])&(relation_df['source']==row['source'])&(relation_df['from_line_number']==row['line_number'])&(((relation_df['from_begin_word_num']==row['begin_word_num']) & (relation_df['to_begin_word_num']==entity_row['begin_word_num'])) |((relation_df['from_begin_word_num']==entity_row['begin_word_num']) & (relation_df['to_begin_word_num']==row['begin_word_num'])))]\n",
    "        \n",
    "        key_to_check = row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(entity_row[\"begin_word_num\"])+\"#\"+str(row[\"begin_word_num\"])\n",
    "        \n",
    "        if(len(relation_df_record)==0):\n",
    "            relation = no_relations_dict[row[\"concept_type\"]]\n",
    "        else:\n",
    "            relation = relation_df_record.iloc[0][\"relation_type\"]\n",
    "        \n",
    "        remaining_relations = [entry for entry in all_relations_dict[entity_row['concept_type']] if entry != relation]\n",
    "        \n",
    "        first_sentence = text_df[text_df['file_name']==row['file_name']].iloc[0]['text'].split(\"\\n\")[row['line_number']-1]\n",
    "        \n",
    "        for entry in sentence_map[relation]:\n",
    "            second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "            \n",
    "            relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        for no_relation in remaining_relations:\n",
    "            for entry in sentence_map[no_relation]:\n",
    "                second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "\n",
    "                relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "                relation_label_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19935"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_rel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9410"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9413"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 3700.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 8076.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 256/256 [00:00<00:00, 8827.65it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_all_text = list()\n",
    "        \n",
    "for file in tqdm(os.listdir(beth_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(beth_file_path+r'\\txt', file)\n",
    "    file_name = file[0:-4]\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'beth'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)\n",
    "    \n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file[0:-4]\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'partners'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)\n",
    "    \n",
    "    \n",
    "for file in tqdm(os.listdir(test_data_texts_path)):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(test_data_texts_path, file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'test_data'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(list_of_all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(text_df['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = random.sample(all_files,int(0.8*len(all_files)))\n",
    "\n",
    "test_files = [entry for entry in all_files if entry not in training_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"blank\":0,\"problem\":1,\"test\":2,\"treatment\":3}\n",
    "\n",
    "encoding_list = list()\n",
    "label_list = list()\n",
    "utility = BERT_utility(get_encodings=True, get_embeddings=False)\n",
    "\n",
    "for file_name in training_files:\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    encoding = utility.encoding_list\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(training_files,open(\"C:/Users/itsma/Documents/CS 6120 Project/training_files.pkl\",\"wb\"))\n",
    "pickle.dump(test_files,open(\"C:/Users/itsma/Documents/CS 6120 Project/test_files.pkl\",\"wb\"))\n",
    "pickle.dump(encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/input_ids.pkl\",\"wb\"))\n",
    "pickle.dump(label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/label.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list = list()\n",
    "label_list = list()\n",
    "utility = BERT_utility()\n",
    "\n",
    "for file in tqdm(os.listdir(beth_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(beth_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='beth')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)\n",
    "\n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        \n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)\n",
    "    \n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        \n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer.encode(\"Hello, my dog is cute\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1,0,2,0,3,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "        \n",
    "outputs = model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[0][0][5].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BERT_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'BERT_utility' from 'C:\\\\Users\\\\itsma\\\\Documents\\\\CS 6120 Project\\\\CS6120\\\\Code\\\\BERT_utility.py'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(BERT_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-52372eac897b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mutility\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERT_utility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#word_list = utility.process_string_finetune(line,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "utility = BERT_utility()\n",
    "#word_list = utility.process_string_finetune(line,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 340/340 [21:13<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "all_words_list = list()\n",
    "\n",
    "utility = BERT_utility(get_embeddings=True,get_encodings=False,use_finetuned_model=True)\n",
    "\n",
    "for file_name in tqdm(training_files):\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[assertion_df['file_name']==file_name]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 86/86 [06:21<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "all_words_list_test = list()\n",
    "\n",
    "utility = BERT_utility(get_embeddings=True,get_encodings=False,use_finetuned_model=True)\n",
    "\n",
    "for file_name in tqdm(test_files):\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[assertion_df['file_name']==file_name]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list_test.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'keyword_vector', 'sentence_index', 'word_index', 'concept',\n",
       "       'assertion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = word_df[word_df['concept']=='problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assertion = np.vstack(list(filt_df[\"keyword_vector\"]))                                \n",
    "y_assertion =list(filt_df[\"assertion\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept = np.vstack(list(word_df[\"keyword_vector\"]))                                \n",
    "y_concept =list(word_df[\"concept\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_concept = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.980297626442755"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_concept.score(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_assertion = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565322515648926"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_assertion.score(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df_test = pd.DataFrame(all_words_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept_test = np.vstack(list(word_df_test[\"keyword_vector\"]))                                \n",
    "y_concept_test =list(word_df_test[\"concept\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805519175716554"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_concept.score(X_concept_test, y_concept_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636040579362416"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_concept = clf_concept.predict(X_concept_test)\n",
    "f1_score(y_concept_test,y_predict_concept,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df_test = word_df_test[word_df_test['concept']=='problem']\n",
    "\n",
    "X_assertion_test = np.vstack(list(filt_df_test[\"keyword_vector\"]))                                \n",
    "y_assertion_test =list(filt_df_test[\"assertion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928894883900183"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_assertion.score(X_assertion_test, y_assertion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_assertion = clf_assertion.predict(X_assertion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205219683677596"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_assertion_test, y_predict_assertion, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 203,    0,    0,    2,   50,   13],\n",
       "       [   0,   20,    0,    0,   74,    0],\n",
       "       [   0,    0,   39,    0,   26,    6],\n",
       "       [  14,    1,    0,  309,  207,   16],\n",
       "       [   9,   13,    1,  115, 7639,   66],\n",
       "       [   6,    1,    2,   15,  101, 1431]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_assertion = ['hypothetical',\n",
    " 'conditional',\n",
    " 'associated_with_someone_else',\n",
    " 'possible',\n",
    " 'present',\n",
    " 'absent']\n",
    "\n",
    "confusion_matrix(y_assertion_test,y_predict_assertion,labels_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[(assertion_df['file_name']==file_name)&(assertion_df['source']=='partners')]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_df[words_df['concept']=='problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(words_df['concept']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"a\":\"1\", \"b\":\"2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept = np.vstack(list(words_df[\"keyword_vector\"]))                                \n",
    "y_concept = words_df[\"concept\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assertion = np.vstack(list(filt_df[\"keyword_vector\"]))                                \n",
    "y_assertion =list(filt_df[\"assertion\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = words_df[words_df['assertion']!='blank'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_concept = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(y_assertion)):\n",
    "    \n",
    "    if(y_assertion[i] not in ['absent', 'associated_with_someone_else', 'conditional', 'hypothetical', 'possible', 'present']):\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_concept.score(X_concept,y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_assertion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion.score(X_assertion,y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data' \n",
    "\n",
    "list_of_all_test_concepts = list()\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\concepts'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\concepts', file)\n",
    "    list_of_all_test_concepts.extend(get_concepts_from_file(file_path,file.strip(\".con\"),'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concept_df = pd.DataFrame(list_of_all_test_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concept_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data' \n",
    "\n",
    "list_of_all_test_assertions = list()\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\ast', file)\n",
    "    list_of_all_test_assertions.extend(get_assertions_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assertion_df = pd.DataFrame(list_of_all_test_assertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assertion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list_test = list()\n",
    "test_data_texts_path =  os.path.dirname(os.getcwd()) + r'\\Data\\test_data'\n",
    "for file in tqdm(os.listdir(test_data_texts_path)):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(test_data_texts_path, file)\n",
    "    \n",
    "    file_name = file.strip(\".txt\")\n",
    "    \n",
    "    oFile = open(file_path, 'r')\n",
    "    \n",
    "    line = oFile.read()\n",
    "    \n",
    "    all_lines = line.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = test_concept_df[(test_concept_df['file_name']==file_name)&(test_concept_df['source']=='test_data')]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = test_assertion_df[(test_assertion_df['file_name']==file_name)&(test_assertion_df['source']=='test_data')]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        \n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            \n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list_test.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_df = pd.DataFrame(all_words_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filt_df = test_word_df[test_word_df['assertion']!='blank'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_concept = np.vstack(list(test_word_df[\"keyword_vector\"]))                                \n",
    "y_test_concept = test_word_df[\"concept\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_assertion = np.vstack(list(test_filt_df[\"keyword_vector\"]))                                \n",
    "y_test_assertion = test_filt_df[\"assertion\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion.score(X_test_assertion,y_test_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_assertion = clf_assertion.predict(X_test_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test_assertion, y_predict_assertion, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_assertion = ['hypothetical',\n",
    " 'conditional',\n",
    " 'associated_with_someone_else',\n",
    " 'possible',\n",
    " 'present',\n",
    " 'absent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['blank', 'problem', 'test', 'treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_assertion,y_predict_assertion,labels_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'headache can be treated Tylenol'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{1} can be treated {0}\".format(\"Tylenol\",\"headache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_predict, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 2381, 1997, 2556, 7355, 1024, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/CS6120/Model/finetuned_model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer.encode(\"Hello, my dog is cute\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "        \n",
    "outputs = finetuned_model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = [entry for entry in label_list if 3 in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/CS6120/Model/finetuned_model.pkl\",\"rb\"))\n",
    "bert_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = bert_tokenizer.encode(\"i love my country\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from BERT_config import bert_config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_config['ncbi_base_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "\n",
    "outputs = bert_model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[1][12][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondab189137550194d64ad595e9dd8ef8b75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
