{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from BERT_utility import BERT_utility\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_concepts = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|t=\"(.*)\"'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            concept_dict = dict()\n",
    "            concept_dict['source'] = source\n",
    "            concept_dict['file_name'] = file_name \n",
    "            concept_dict['text'] = pattern_search.group(1)\n",
    "            concept_dict['line_number'] = int(pattern_search.group(2))\n",
    "            concept_dict['begin_word_num'] = int(pattern_search.group(3))\n",
    "            concept_dict['end_word_num'] = int(pattern_search.group(4))\n",
    "            concept_dict['concept_type'] = pattern_search.group(5)\n",
    "            list_of_concepts.append(concept_dict)\n",
    "    \n",
    "    return list_of_concepts\n",
    "\n",
    "def get_assertions_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_assertions = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|t=\".*\"\\|\\|a=\"(.*)\"'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            assertion_dict = dict()\n",
    "            assertion_dict['source'] = source\n",
    "            assertion_dict['file_name'] = file_name \n",
    "            assertion_dict['text'] = pattern_search.group(1)\n",
    "            assertion_dict['line_number'] = int(pattern_search.group(2))\n",
    "            assertion_dict['begin_word_num'] = int(pattern_search.group(3))\n",
    "            assertion_dict['end_word_num'] = int(pattern_search.group(4))\n",
    "            assertion_dict['assertion_type'] = pattern_search.group(5)\n",
    "            list_of_assertions.append(assertion_dict)\n",
    "    \n",
    "    return list_of_assertions\n",
    "\n",
    "def get_relations_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_relations = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|r=\"(.*)\"\\|\\|c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            relation_dict = dict()\n",
    "            relation_dict['source'] = source\n",
    "            relation_dict['file_name'] = file_name \n",
    "            relation_dict['from_text'] = pattern_search.group(1)\n",
    "            relation_dict['from_line_number'] = int(pattern_search.group(2))\n",
    "            relation_dict['from_begin_word_num'] = int(pattern_search.group(3))\n",
    "            relation_dict['from_end_word_num'] = int(pattern_search.group(4))\n",
    "            relation_dict['relation_type'] = pattern_search.group(5)\n",
    "            relation_dict['to_text'] = pattern_search.group(6)\n",
    "            relation_dict['to_line_number'] = int(pattern_search.group(7))\n",
    "            relation_dict['to_begin_word_num'] = int(pattern_search.group(8))\n",
    "            relation_dict['to_end_word_num'] = int(pattern_search.group(9))\n",
    "            list_of_relations.append(relation_dict)\n",
    "    \n",
    "    return list_of_relations\n",
    "\n",
    "def create_pos_dict_concept(concept):\n",
    "    positions = dict()\n",
    "    \n",
    "    for index,row in concept.iterrows():\n",
    "        for i in range(row['begin_word_num'],row['end_word_num']+1):\n",
    "            positions[str(row['line_number'])+\":\"+str(i)] = row['concept_type']\n",
    "    \n",
    "    return positions\n",
    "\n",
    "def create_pos_dict_assertion(assertion):\n",
    "    positions = dict()\n",
    "    \n",
    "    for index,row in assertion.iterrows():\n",
    "        for i in range(row['begin_word_num'],row['end_word_num']+1):\n",
    "            positions[str(row['line_number'])+\":\"+str(i)] = row['assertion_type']\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\concept_assertion_relation_training_data' \n",
    "beth_file_path = data_file_path + r'\\beth'\n",
    "partners_file_path = data_file_path + r'\\partners'\n",
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data'\n",
    "test_data_texts_path =  os.path.dirname(os.getcwd()) + r'\\Data\\test_data'\n",
    "\n",
    "list_of_all_concepts = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\concept'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\concept', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\concept'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\concept', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\concepts'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\concepts', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.DataFrame(list_of_all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>begin_word_num</th>\n",
       "      <th>end_word_num</th>\n",
       "      <th>concept_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>left basilar atelectasis</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>ventral hernia</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>htn</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>spontaneous echo contrast</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>cath</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name                       text  line_number  begin_word_num  \\\n",
       "0   beth  record-105   left basilar atelectasis           55               6   \n",
       "1   beth  record-105             ventral hernia          143               1   \n",
       "2   beth  record-105                        htn           26               0   \n",
       "3   beth  record-105  spontaneous echo contrast           68               1   \n",
       "4   beth  record-105                       cath           21               0   \n",
       "\n",
       "   end_word_num concept_type  \n",
       "0             8      problem  \n",
       "1             2      problem  \n",
       "2             0      problem  \n",
       "3             3      problem  \n",
       "4             0         test  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concept_type\n",
       "problem      19665\n",
       "test         13833\n",
       "treatment    14188\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df.groupby(['concept_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_assertions = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_df = pd.DataFrame(list_of_all_assertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>begin_word_num</th>\n",
       "      <th>end_word_num</th>\n",
       "      <th>assertion_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>left basilar atelectasis</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>ventral hernia</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>htn</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>spontaneous echo contrast</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>80% lm lesion</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name                       text  line_number  begin_word_num  \\\n",
       "0   beth  record-105   left basilar atelectasis           55               6   \n",
       "1   beth  record-105             ventral hernia          143               1   \n",
       "2   beth  record-105                        htn           26               0   \n",
       "3   beth  record-105  spontaneous echo contrast           68               1   \n",
       "4   beth  record-105              80% lm lesion           21               6   \n",
       "\n",
       "   end_word_num assertion_type  \n",
       "0             8        present  \n",
       "1             2        present  \n",
       "2             0        present  \n",
       "3             3         absent  \n",
       "4             8        present  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assertion_type\n",
       "absent                           4190\n",
       "associated_with_someone_else      220\n",
       "conditional                       221\n",
       "hypothetical                      827\n",
       "possible                          961\n",
       "present                         13246\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertion_df.groupby(['assertion_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conditional',\n",
       " 'present',\n",
       " 'possible',\n",
       " 'associated_with_someone_else',\n",
       " 'hypothetical',\n",
       " 'absent']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(list(assertion_df['assertion_type'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_relations = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_df = pd.DataFrame(list_of_all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>from_text</th>\n",
       "      <th>from_line_number</th>\n",
       "      <th>from_begin_word_num</th>\n",
       "      <th>from_end_word_num</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>to_text</th>\n",
       "      <th>to_line_number</th>\n",
       "      <th>to_begin_word_num</th>\n",
       "      <th>to_end_word_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>cath</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TeRP</td>\n",
       "      <td>80% lm lesion</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>pefusion imaging</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>TeRP</td>\n",
       "      <td>perfusion defects</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>drugs</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>TrCP</td>\n",
       "      <td>known allergies</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>metal plate</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>TrAP</td>\n",
       "      <td>gsw</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>creams</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>TrNAP</td>\n",
       "      <td>any incisions</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name         from_text  from_line_number  from_begin_word_num  \\\n",
       "0   beth  record-105              cath                21                    0   \n",
       "1   beth  record-105  pefusion imaging                19                    6   \n",
       "2   beth  record-105             drugs                12                    8   \n",
       "3   beth  record-105       metal plate                26                    7   \n",
       "4   beth  record-105            creams               145                   14   \n",
       "\n",
       "   from_end_word_num relation_type            to_text  to_line_number  \\\n",
       "0                  0          TeRP      80% lm lesion              21   \n",
       "1                  7          TeRP  perfusion defects              19   \n",
       "2                  8          TrCP    known allergies              12   \n",
       "3                  8          TrAP                gsw              26   \n",
       "4                 14         TrNAP      any incisions             145   \n",
       "\n",
       "   to_begin_word_num  to_end_word_num  \n",
       "0                  6                8  \n",
       "1                 12               13  \n",
       "2                  5                6  \n",
       "3                 11               11  \n",
       "4                 20               21  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relation_type\n",
       "PIP      2203\n",
       "TeCP      504\n",
       "TeRP     3053\n",
       "TrAP     2617\n",
       "TrCP      526\n",
       "TrIP      203\n",
       "TrNAP     174\n",
       "TrWP      133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_df.groupby(['relation_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_map = dict()\n",
    "\n",
    "#Treatment improves medical problem\n",
    "sentence_map['TrIP'] = [\"{0} improves {1}.\", \"{1} can be treated by {0}.\"]\n",
    "\n",
    "#Treatment worsens medical problem\n",
    "sentence_map['TrWP'] = [\"{0} worsens {1}.\", \"{0} does not improve {1}.\", \"{0} does not cure {1}.\"]\n",
    "\n",
    "#Treatment causes medical problem\n",
    "sentence_map['TrCP'] = [\"{0} causes {1}.\", \"{0} results in {1}.\", \"{1} is a result of {0}.\"]\n",
    "\n",
    "#Treatment is administered for the medical problem\n",
    "sentence_map['TrAP'] = [\"{0} is prescribed for {1}.\", \"{0} is administered for {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['TrNAP'] = [\"{0} can not be prescribed due to {1}.\", \"{0} is not administered due to {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['TrNRP'] = [\"{0} and {1} has no relation.\"]\n",
    "\n",
    "#Test reveals medical problem\n",
    "sentence_map['TeRP'] = [\"{0} reveals {1}.\", \"{0} indicates {1}.\"]\n",
    "\n",
    "#Test conducted to investigate medical problem\n",
    "sentence_map['TeCP'] = [\"{0} is conducted to check {1}.\", \"{0} is performed to investigate {1}.\"]\n",
    "\n",
    "#Test and problem has no relation\n",
    "sentence_map['TeNRP'] = [\"{0} and {1} has no relation.\"]\n",
    "\n",
    "#Medical problem indicates medical problem\n",
    "sentence_map['PIP'] = [\"{1} can cause {0}.\", \"{0} is a result of {1}\"]\n",
    "\n",
    "#Medical problem indicates medical problem\n",
    "sentence_map['PNP'] = [\"{0} and {1} has no relation.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_utility = BERT_utility()\n",
    "\n",
    "relation_encoding_list = list()\n",
    "relation_label_list = list()\n",
    "\n",
    "no_rel_count = 0\n",
    "rel_count = 0\n",
    "exist_rec_list = list()\n",
    "\n",
    "all_relations_dict = {\"treatment\":['TrIP','TrWP','TrCP','TrAP',\"TrNRP\"], \"test\":['TeRP','TeCP','TeNRP'], \"problem\":['PIP','PNP']}\n",
    "no_relations_dict = {\"treatment\":\"TrNRP\", \"test\":\"TeNRP\", \"problem\":\"PNP\"}\n",
    "\n",
    "all_problems = concept_df[(concept_df['concept_type']=='problem') & (concept_df['file_name'].isin(training_files))]\n",
    "\n",
    "for index,row in all_problems.iterrows():\n",
    "    all_other_entities = concept_df[(concept_df['file_name']==row['file_name'])&(concept_df['source']==row['source'])&(concept_df['line_number']==row['line_number'])&(concept_df['begin_word_num']!=row['begin_word_num'])]\n",
    "    \n",
    "    for entity_index,entity_row in all_other_entities.iterrows():\n",
    "        \n",
    "        key_to_check = row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(entity_row[\"begin_word_num\"])+\"#\"+str(row[\"begin_word_num\"])\n",
    "        \n",
    "        if(key_to_check in exist_rec_list):\n",
    "            continue\n",
    "        else:\n",
    "            exist_rec_list.append(row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(row[\"begin_word_num\"])+\"#\"+str(entity_row[\"begin_word_num\"]))\n",
    "        \n",
    "        relation_df_record = relation_df[(relation_df['file_name']==row['file_name'])&(relation_df['source']==row['source'])&(relation_df['from_line_number']==row['line_number'])&(((relation_df['from_begin_word_num']==row['begin_word_num']) & (relation_df['to_begin_word_num']==entity_row['begin_word_num'])) |((relation_df['from_begin_word_num']==entity_row['begin_word_num']) & (relation_df['to_begin_word_num']==row['begin_word_num'])))]\n",
    "        \n",
    "        if(len(relation_df_record)==0):\n",
    "            relation = no_relations_dict[entity_row[\"concept_type\"]]\n",
    "        else:\n",
    "            relation = relation_df_record.iloc[0][\"relation_type\"]\n",
    "        \n",
    "        remaining_relations = [entry for entry in all_relations_dict[entity_row['concept_type']] if entry != relation]\n",
    "        \n",
    "        first_sentence = text_df[text_df['file_name']==row['file_name']].iloc[0]['text'].split(\"\\n\")[row['line_number']-1]\n",
    "        \n",
    "        for entry in sentence_map[relation]:\n",
    "            second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "            \n",
    "            relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        for no_relation in remaining_relations:\n",
    "            for entry in sentence_map[no_relation]:\n",
    "                second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "\n",
    "                relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "                relation_label_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list = [entry[0] for entry in relation_encoding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list = [list(np.array(entry)) for entry in relation_encoding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list = list(relation_encoding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39414"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ entry for entry in relation_label_list if entry==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19935"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_rel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9410"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9413"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 3700.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 8076.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 256/256 [00:00<00:00, 8827.65it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_all_text = list()\n",
    "        \n",
    "for file in tqdm(os.listdir(beth_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(beth_file_path+r'\\txt', file)\n",
    "    file_name = file[0:-4]\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'beth'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)\n",
    "    \n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file[0:-4]\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'partners'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)\n",
    "    \n",
    "    \n",
    "for file in tqdm(os.listdir(test_data_texts_path)):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(test_data_texts_path, file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'test_data'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(list_of_all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(text_df['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = random.sample(all_files,int(0.8*len(all_files)))\n",
    "\n",
    "test_files = [entry for entry in all_files if entry not in training_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"blank\":0,\"problem\":1,\"test\":2,\"treatment\":3}\n",
    "\n",
    "encoding_list = list()\n",
    "label_list = list()\n",
    "utility = BERT_utility(get_encodings=True, get_embeddings=False)\n",
    "\n",
    "for file_name in training_files:\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    encoding = utility.encoding_list\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(training_files,open(\"C:/Users/itsma/Documents/CS 6120 Project/training_files.pkl\",\"wb\"))\n",
    "pickle.dump(test_files,open(\"C:/Users/itsma/Documents/CS 6120 Project/test_files.pkl\",\"wb\"))\n",
    "pickle.dump(encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/input_ids.pkl\",\"wb\"))\n",
    "pickle.dump(label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/label.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/input_ids.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0098',\n",
       " '0389',\n",
       " '0006',\n",
       " '0273',\n",
       " '0297',\n",
       " '0213',\n",
       " '270045381',\n",
       " '959086752',\n",
       " '622086964',\n",
       " '0130',\n",
       " 'record-31',\n",
       " '0225',\n",
       " '337702516_WGH',\n",
       " '297228405_DH',\n",
       " '105732749',\n",
       " '0301',\n",
       " '0217',\n",
       " '825330116',\n",
       " '655358166_WGH',\n",
       " 'record-121',\n",
       " 'record-55',\n",
       " '596437842',\n",
       " 'record-49',\n",
       " '0374',\n",
       " 'record-33',\n",
       " '0285',\n",
       " '0209',\n",
       " '0473',\n",
       " '0177',\n",
       " '405507617',\n",
       " '817406016_RWH',\n",
       " '0025',\n",
       " '044687343_ELMVH',\n",
       " 'record-19',\n",
       " '641557794_WGH',\n",
       " 'record-73',\n",
       " '0369',\n",
       " '0141',\n",
       " '0382',\n",
       " '0085',\n",
       " '614746156',\n",
       " '0022',\n",
       " '0365',\n",
       " 'record-67',\n",
       " '0434',\n",
       " '0310',\n",
       " '0222',\n",
       " '405868244_YC',\n",
       " 'record-18',\n",
       " 'record-38',\n",
       " '503651854_WGH',\n",
       " '0476',\n",
       " '0366',\n",
       " '0134',\n",
       " 'record-29',\n",
       " 'record-52',\n",
       " '814743340_RWH',\n",
       " '143748600_SC',\n",
       " '0318',\n",
       " '0077',\n",
       " 'record-28',\n",
       " '0437',\n",
       " '0114',\n",
       " '115026438_SC',\n",
       " '0377',\n",
       " '0105',\n",
       " '0309',\n",
       " '0229',\n",
       " '0461',\n",
       " '0322',\n",
       " '708739405_DH',\n",
       " '0205',\n",
       " 'record-59',\n",
       " '544907529_RWH',\n",
       " '0282',\n",
       " '0149',\n",
       " '0090',\n",
       " '0277',\n",
       " '0050',\n",
       " '0398',\n",
       " '0001',\n",
       " '869436718_SC',\n",
       " 'record-82',\n",
       " '0049',\n",
       " '223159990',\n",
       " '388755206_SC',\n",
       " '745431641_WGH',\n",
       " '767751445_ELMVH',\n",
       " '0233',\n",
       " '289811204',\n",
       " 'record-70',\n",
       " '0202',\n",
       " '0442',\n",
       " '0046',\n",
       " '0081',\n",
       " '0370',\n",
       " 'record-24',\n",
       " '245317863_WGH',\n",
       " 'record-141',\n",
       " 'record-45',\n",
       " '0065',\n",
       " 'record-56',\n",
       " '891864133_RWH',\n",
       " '455343475_PUMC',\n",
       " '699905656_SC',\n",
       " '0469',\n",
       " 'record-22',\n",
       " '0409',\n",
       " '306968218_YC',\n",
       " '188543380',\n",
       " '0198',\n",
       " '0397',\n",
       " 'record-17',\n",
       " '0467',\n",
       " '779878634',\n",
       " '0421',\n",
       " '0013',\n",
       " 'record-13',\n",
       " '0350',\n",
       " '853262744',\n",
       " '0137',\n",
       " '0302',\n",
       " '517414339',\n",
       " '0070',\n",
       " '0017',\n",
       " '0445',\n",
       " '0415',\n",
       " '0261',\n",
       " '493597270',\n",
       " '788268693_DH',\n",
       " '0401',\n",
       " '0286',\n",
       " '688127038_EH',\n",
       " '0410',\n",
       " '346176858_SC',\n",
       " '176746010_WGH',\n",
       " '0061',\n",
       " '0361',\n",
       " '0460',\n",
       " '0357',\n",
       " '0246',\n",
       " '0185',\n",
       " 'record-16',\n",
       " '0353',\n",
       " '0178',\n",
       " '0289',\n",
       " '0073',\n",
       " '0121',\n",
       " '0354',\n",
       " '0053',\n",
       " '0455',\n",
       " '910458031',\n",
       " '0157',\n",
       " '0054',\n",
       " '979440029_RWH',\n",
       " '0337',\n",
       " 'record-25',\n",
       " '333521954',\n",
       " 'record-107',\n",
       " '0385',\n",
       " '0294',\n",
       " 'record-176',\n",
       " '0349',\n",
       " '0317',\n",
       " '0245',\n",
       " '0458',\n",
       " '348301810',\n",
       " '0457',\n",
       " '0373',\n",
       " '0378',\n",
       " '0342',\n",
       " '245096078',\n",
       " '0158',\n",
       " '412141256',\n",
       " '0193',\n",
       " '0298',\n",
       " '837898389',\n",
       " '0329',\n",
       " '0189',\n",
       " '0464',\n",
       " 'record-175',\n",
       " '0475',\n",
       " 'record-178',\n",
       " 'record-20',\n",
       " '0102',\n",
       " '0106',\n",
       " '0325',\n",
       " '0074',\n",
       " '0113',\n",
       " '0358',\n",
       " '0146',\n",
       " 'record-14',\n",
       " 'record-58',\n",
       " '425680098_SC',\n",
       " '0452',\n",
       " '523704694',\n",
       " '0066',\n",
       " '0412',\n",
       " 'record-26',\n",
       " '0281',\n",
       " '0474',\n",
       " '0045',\n",
       " '932057504_DH',\n",
       " '0150',\n",
       " '0190',\n",
       " '0265',\n",
       " '0238',\n",
       " '917989835_RWH',\n",
       " '0101',\n",
       " '0436',\n",
       " '0165',\n",
       " '0097',\n",
       " '0463',\n",
       " '0145',\n",
       " '693008750',\n",
       " 'record-15',\n",
       " '0029',\n",
       " '332803550',\n",
       " '0390',\n",
       " 'record-66',\n",
       " '0242',\n",
       " '843566350_RWH',\n",
       " '0166',\n",
       " '839999049_YC',\n",
       " '0334',\n",
       " '0201',\n",
       " '0439',\n",
       " '0427',\n",
       " '598403789_DH',\n",
       " '0449',\n",
       " '0169',\n",
       " '433651389',\n",
       " '0086',\n",
       " 'record-36',\n",
       " '262182942',\n",
       " '0333',\n",
       " '274230067_EH',\n",
       " '0221',\n",
       " '0249',\n",
       " '0018',\n",
       " '989519730_WGH',\n",
       " '0117',\n",
       " '0431',\n",
       " '0142',\n",
       " '0129',\n",
       " 'record-27',\n",
       " '0026',\n",
       " '0186',\n",
       " '0058',\n",
       " '0237',\n",
       " '0448',\n",
       " '0182',\n",
       " '0381',\n",
       " '0314',\n",
       " '0194',\n",
       " '0446',\n",
       " '627258104',\n",
       " '0109',\n",
       " '0250',\n",
       " '0078',\n",
       " 'record-50',\n",
       " '0197',\n",
       " '0258',\n",
       " '0206',\n",
       " '0305',\n",
       " '0226',\n",
       " '037945397_RWH',\n",
       " '0094',\n",
       " '424729395_DH',\n",
       " 'record-48',\n",
       " '262912613',\n",
       " '0418',\n",
       " '0030',\n",
       " '0293',\n",
       " '0173',\n",
       " '0033',\n",
       " '0214',\n",
       " '0038',\n",
       " 'record-144',\n",
       " 'record-84',\n",
       " 'record-53',\n",
       " 'record-30',\n",
       " '384729825',\n",
       " '0009',\n",
       " 'record-80',\n",
       " '0468',\n",
       " '0122',\n",
       " '0138',\n",
       " '0181',\n",
       " '0021',\n",
       " '402389409_WGH',\n",
       " '0262',\n",
       " '0269',\n",
       " '0433',\n",
       " '0386',\n",
       " '0153',\n",
       " '0451',\n",
       " '0034',\n",
       " 'record-74',\n",
       " 'record-122',\n",
       " '0346',\n",
       " '0440',\n",
       " '0089',\n",
       " '0424',\n",
       " '498710998',\n",
       " '0254',\n",
       " '0110',\n",
       " 'record-37',\n",
       " '320422564',\n",
       " '026350193_RWH',\n",
       " '212512774_WGH',\n",
       " '0443',\n",
       " '0326',\n",
       " 'record-51',\n",
       " '0362',\n",
       " '965367286_WGH',\n",
       " '0057',\n",
       " '920798564',\n",
       " 'record-140',\n",
       " '0174',\n",
       " '0413',\n",
       " 'record-71',\n",
       " '101407944_PUMC',\n",
       " 'record-23',\n",
       " '0402',\n",
       " '0133',\n",
       " '0270',\n",
       " '0416',\n",
       " '555509347_PUMC',\n",
       " '0126',\n",
       " '0266',\n",
       " '0477',\n",
       " 'record-124',\n",
       " '176318078_a',\n",
       " '0290',\n",
       " 'record-83',\n",
       " '0093',\n",
       " '0241',\n",
       " '0154',\n",
       " '0430']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(relation_encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/relation_encodings.pkl\",\"wb\"))\n",
    "pickle.dump(relation_label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/relation_labels.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list = list()\n",
    "label_list = list()\n",
    "utility = BERT_utility()\n",
    "\n",
    "for file in tqdm(os.listdir(beth_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(beth_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='beth')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)\n",
    "\n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        \n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)\n",
    "    \n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        \n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer.encode(\"Hello, my dog is cute\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1,0,2,0,3,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "        \n",
    "outputs = model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[0][0][5].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BERT_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'BERT_utility' from 'C:\\\\Users\\\\itsma\\\\Documents\\\\CS 6120 Project\\\\CS6120\\\\Code\\\\BERT_utility.py'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(BERT_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-52372eac897b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mutility\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERT_utility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#word_list = utility.process_string_finetune(line,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "utility = BERT_utility()\n",
    "#word_list = utility.process_string_finetune(line,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▍                                                          | 94/340 [07:27<13:40,  3.33s/it]"
     ]
    }
   ],
   "source": [
    "all_words_list = list()\n",
    "\n",
    "utility = BERT_utility(get_embeddings=True,get_encodings=False,use_finetuned_model=True)\n",
    "\n",
    "for file_name in tqdm(training_files):\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[assertion_df['file_name']==file_name]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 86/86 [06:21<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "all_words_list_test = list()\n",
    "\n",
    "utility = BERT_utility(get_embeddings=True,get_encodings=False,use_finetuned_model=True)\n",
    "\n",
    "for file_name in tqdm(test_files):\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[assertion_df['file_name']==file_name]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list_test.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'keyword_vector', 'sentence_index', 'word_index', 'concept',\n",
       "       'assertion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = word_df[word_df['concept']=='problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assertion = np.vstack(list(filt_df[\"keyword_vector\"]))                                \n",
    "y_assertion =list(filt_df[\"assertion\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept = np.vstack(list(word_df[\"keyword_vector\"]))                                \n",
    "y_concept =list(word_df[\"concept\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_concept = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.980297626442755"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_concept.score(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_assertion = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565322515648926"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_assertion.score(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df_test = pd.DataFrame(all_words_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept_test = np.vstack(list(word_df_test[\"keyword_vector\"]))                                \n",
    "y_concept_test =list(word_df_test[\"concept\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805519175716554"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_concept.score(X_concept_test, y_concept_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636040579362416"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_concept = clf_concept.predict(X_concept_test)\n",
    "f1_score(y_concept_test,y_predict_concept,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df_test = word_df_test[word_df_test['concept']=='problem']\n",
    "\n",
    "X_assertion_test = np.vstack(list(filt_df_test[\"keyword_vector\"]))                                \n",
    "y_assertion_test =list(filt_df_test[\"assertion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928894883900183"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_assertion.score(X_assertion_test, y_assertion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_assertion = clf_assertion.predict(X_assertion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205219683677596"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_assertion_test, y_predict_assertion, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 203,    0,    0,    2,   50,   13],\n",
       "       [   0,   20,    0,    0,   74,    0],\n",
       "       [   0,    0,   39,    0,   26,    6],\n",
       "       [  14,    1,    0,  309,  207,   16],\n",
       "       [   9,   13,    1,  115, 7639,   66],\n",
       "       [   6,    1,    2,   15,  101, 1431]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_assertion = ['hypothetical',\n",
    " 'conditional',\n",
    " 'associated_with_someone_else',\n",
    " 'possible',\n",
    " 'present',\n",
    " 'absent']\n",
    "\n",
    "confusion_matrix(y_assertion_test,y_predict_assertion,labels_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[(assertion_df['file_name']==file_name)&(assertion_df['source']=='partners')]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_df[words_df['concept']=='problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(words_df['concept']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"a\":\"1\", \"b\":\"2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept = np.vstack(list(words_df[\"keyword_vector\"]))                                \n",
    "y_concept = words_df[\"concept\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assertion = np.vstack(list(filt_df[\"keyword_vector\"]))                                \n",
    "y_assertion =list(filt_df[\"assertion\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = words_df[words_df['assertion']!='blank'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_concept = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(y_assertion)):\n",
    "    \n",
    "    if(y_assertion[i] not in ['absent', 'associated_with_someone_else', 'conditional', 'hypothetical', 'possible', 'present']):\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_concept.score(X_concept,y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_assertion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion.score(X_assertion,y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data' \n",
    "\n",
    "list_of_all_test_concepts = list()\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\concepts'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\concepts', file)\n",
    "    list_of_all_test_concepts.extend(get_concepts_from_file(file_path,file.strip(\".con\"),'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concept_df = pd.DataFrame(list_of_all_test_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concept_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data' \n",
    "\n",
    "list_of_all_test_assertions = list()\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\ast', file)\n",
    "    list_of_all_test_assertions.extend(get_assertions_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assertion_df = pd.DataFrame(list_of_all_test_assertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assertion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list_test = list()\n",
    "test_data_texts_path =  os.path.dirname(os.getcwd()) + r'\\Data\\test_data'\n",
    "for file in tqdm(os.listdir(test_data_texts_path)):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(test_data_texts_path, file)\n",
    "    \n",
    "    file_name = file.strip(\".txt\")\n",
    "    \n",
    "    oFile = open(file_path, 'r')\n",
    "    \n",
    "    line = oFile.read()\n",
    "    \n",
    "    all_lines = line.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = test_concept_df[(test_concept_df['file_name']==file_name)&(test_concept_df['source']=='test_data')]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = test_assertion_df[(test_assertion_df['file_name']==file_name)&(test_assertion_df['source']=='test_data')]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        \n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            \n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list_test.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_df = pd.DataFrame(all_words_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filt_df = test_word_df[test_word_df['assertion']!='blank'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_concept = np.vstack(list(test_word_df[\"keyword_vector\"]))                                \n",
    "y_test_concept = test_word_df[\"concept\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_assertion = np.vstack(list(test_filt_df[\"keyword_vector\"]))                                \n",
    "y_test_assertion = test_filt_df[\"assertion\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion.score(X_test_assertion,y_test_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_assertion = clf_assertion.predict(X_test_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test_assertion, y_predict_assertion, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_assertion = ['hypothetical',\n",
    " 'conditional',\n",
    " 'associated_with_someone_else',\n",
    " 'possible',\n",
    " 'present',\n",
    " 'absent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['blank', 'problem', 'test', 'treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_assertion,y_predict_assertion,labels_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'headache can be treated Tylenol'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{1} can be treated {0}\".format(\"Tylenol\",\"headache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_predict, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 2381, 1997, 2556, 7355, 1024, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/CS6120/Model/finetuned_model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer.encode(\"Hello, my dog is cute\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "        \n",
    "outputs = finetuned_model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = [entry for entry in label_list if 3 in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/CS6120/Model/finetuned_model.pkl\",\"rb\"))\n",
    "bert_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = bert_tokenizer.encode(\"i love my country\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from BERT_config import bert_config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_config['ncbi_base_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "\n",
    "outputs = bert_model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[1][12][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondab189137550194d64ad595e9dd8ef8b75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
