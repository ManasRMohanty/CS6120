{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from BERT_utility import BERT_utility\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_concepts = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|t=\"(.*)\"'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            concept_dict = dict()\n",
    "            concept_dict['source'] = source\n",
    "            concept_dict['file_name'] = file_name \n",
    "            concept_dict['text'] = pattern_search.group(1)\n",
    "            concept_dict['line_number'] = int(pattern_search.group(2))\n",
    "            concept_dict['begin_word_num'] = int(pattern_search.group(3))\n",
    "            concept_dict['end_word_num'] = int(pattern_search.group(4))\n",
    "            concept_dict['concept_type'] = pattern_search.group(5)\n",
    "            list_of_concepts.append(concept_dict)\n",
    "    \n",
    "    return list_of_concepts\n",
    "\n",
    "def get_assertions_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_assertions = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|t=\".*\"\\|\\|a=\"(.*)\"'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            assertion_dict = dict()\n",
    "            assertion_dict['source'] = source\n",
    "            assertion_dict['file_name'] = file_name \n",
    "            assertion_dict['text'] = pattern_search.group(1)\n",
    "            assertion_dict['line_number'] = int(pattern_search.group(2))\n",
    "            assertion_dict['begin_word_num'] = int(pattern_search.group(3))\n",
    "            assertion_dict['end_word_num'] = int(pattern_search.group(4))\n",
    "            assertion_dict['assertion_type'] = pattern_search.group(5)\n",
    "            list_of_assertions.append(assertion_dict)\n",
    "    \n",
    "    return list_of_assertions\n",
    "\n",
    "def get_relations_from_file(file_path,file_name,source):\n",
    "    \n",
    "    list_of_relations = list()\n",
    "    \n",
    "    file = open(file_path, 'r',encoding=\"utf8\",errors = 'ignore') \n",
    "    Lines = file.readlines() \n",
    "    \n",
    "    for line in Lines: \n",
    "        entry = line.strip()\n",
    "        regular_exp_con = 'c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)\\|\\|r=\"(.*)\"\\|\\|c=\"(.*)\" ([0-9]*):([0-9]*) [0-9]*:([0-9]*)'\n",
    "        pattern_search = re.search(regular_exp_con, entry, re.IGNORECASE)\n",
    "\n",
    "        if pattern_search:\n",
    "            relation_dict = dict()\n",
    "            relation_dict['source'] = source\n",
    "            relation_dict['file_name'] = file_name \n",
    "            relation_dict['from_text'] = pattern_search.group(1)\n",
    "            relation_dict['from_line_number'] = int(pattern_search.group(2))\n",
    "            relation_dict['from_begin_word_num'] = int(pattern_search.group(3))\n",
    "            relation_dict['from_end_word_num'] = int(pattern_search.group(4))\n",
    "            relation_dict['relation_type'] = pattern_search.group(5)\n",
    "            relation_dict['to_text'] = pattern_search.group(6)\n",
    "            relation_dict['to_line_number'] = int(pattern_search.group(7))\n",
    "            relation_dict['to_begin_word_num'] = int(pattern_search.group(8))\n",
    "            relation_dict['to_end_word_num'] = int(pattern_search.group(9))\n",
    "            list_of_relations.append(relation_dict)\n",
    "    \n",
    "    return list_of_relations\n",
    "\n",
    "def create_pos_dict_concept(concept):\n",
    "    positions = dict()\n",
    "    \n",
    "    for index,row in concept.iterrows():\n",
    "        for i in range(row['begin_word_num'],row['end_word_num']+1):\n",
    "            positions[str(row['line_number'])+\":\"+str(i)] = row['concept_type']\n",
    "    \n",
    "    return positions\n",
    "\n",
    "def create_pos_dict_assertion(assertion):\n",
    "    positions = dict()\n",
    "    \n",
    "    for index,row in assertion.iterrows():\n",
    "        for i in range(row['begin_word_num'],row['end_word_num']+1):\n",
    "            positions[str(row['line_number'])+\":\"+str(i)] = row['assertion_type']\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\concept_assertion_relation_training_data' \n",
    "beth_file_path = data_file_path + r'\\beth'\n",
    "partners_file_path = data_file_path + r'\\partners'\n",
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data'\n",
    "test_data_texts_path =  os.path.dirname(os.getcwd()) + r'\\Data\\test_data'\n",
    "\n",
    "list_of_all_concepts = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\concept'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\concept', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\concept'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\concept', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\concepts'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\concepts', file)\n",
    "    list_of_all_concepts.extend(get_concepts_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.DataFrame(list_of_all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>begin_word_num</th>\n",
       "      <th>end_word_num</th>\n",
       "      <th>concept_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>left basilar atelectasis</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>ventral hernia</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>htn</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>spontaneous echo contrast</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>cath</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name                       text  line_number  begin_word_num  \\\n",
       "0   beth  record-105   left basilar atelectasis           55               6   \n",
       "1   beth  record-105             ventral hernia          143               1   \n",
       "2   beth  record-105                        htn           26               0   \n",
       "3   beth  record-105  spontaneous echo contrast           68               1   \n",
       "4   beth  record-105                       cath           21               0   \n",
       "\n",
       "   end_word_num concept_type  \n",
       "0             8      problem  \n",
       "1             2      problem  \n",
       "2             0      problem  \n",
       "3             3      problem  \n",
       "4             0         test  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>begin_word_num</th>\n",
       "      <th>end_word_num</th>\n",
       "      <th>concept_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>right facial plating</td>\n",
       "      <td>143</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>treatment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>htn</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>right nephrectomy</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>treatment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source   file_name                  text  line_number  begin_word_num  \\\n",
       "10   beth  record-105  right facial plating          143               6   \n",
       "11   beth  record-105                   htn          143               0   \n",
       "31   beth  record-105     right nephrectomy          143               3   \n",
       "\n",
       "    end_word_num concept_type  \n",
       "10             8    treatment  \n",
       "11             0      problem  \n",
       "31             4    treatment  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df[(concept_df['file_name']=='record-105')&(concept_df['source']=='beth')&(concept_df['line_number']==143)&(concept_df['begin_word_num']!=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "concept_type\n",
       "problem      19665\n",
       "test         13833\n",
       "treatment    14188\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_df.groupby(['concept_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_assertions = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\ast', file)\n",
    "    list_of_all_assertions.extend(get_assertions_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertion_df = pd.DataFrame(list_of_all_assertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>begin_word_num</th>\n",
       "      <th>end_word_num</th>\n",
       "      <th>assertion_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>left basilar atelectasis</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>ventral hernia</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>htn</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>spontaneous echo contrast</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>80% lm lesion</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name                       text  line_number  begin_word_num  \\\n",
       "0   beth  record-105   left basilar atelectasis           55               6   \n",
       "1   beth  record-105             ventral hernia          143               1   \n",
       "2   beth  record-105                        htn           26               0   \n",
       "3   beth  record-105  spontaneous echo contrast           68               1   \n",
       "4   beth  record-105              80% lm lesion           21               6   \n",
       "\n",
       "   end_word_num assertion_type  \n",
       "0             8        present  \n",
       "1             2        present  \n",
       "2             0        present  \n",
       "3             3         absent  \n",
       "4             8        present  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assertion_type\n",
       "absent                           4190\n",
       "associated_with_someone_else      220\n",
       "conditional                       221\n",
       "hypothetical                      827\n",
       "possible                          961\n",
       "present                         13246\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertion_df.groupby(['assertion_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hypothetical',\n",
       " 'conditional',\n",
       " 'absent',\n",
       " 'possible',\n",
       " 'present',\n",
       " 'associated_with_someone_else']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(list(assertion_df['assertion_type'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_relations = list()\n",
    "\n",
    "for file in os.listdir(beth_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(beth_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'beth'))\n",
    "\n",
    "for file in os.listdir(partners_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(partners_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'partners'))\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\rel'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\rel', file)\n",
    "    list_of_all_relations.extend(get_relations_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_df = pd.DataFrame(list_of_all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>from_text</th>\n",
       "      <th>from_line_number</th>\n",
       "      <th>from_begin_word_num</th>\n",
       "      <th>from_end_word_num</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>to_text</th>\n",
       "      <th>to_line_number</th>\n",
       "      <th>to_begin_word_num</th>\n",
       "      <th>to_end_word_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>cath</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TeRP</td>\n",
       "      <td>80% lm lesion</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>pefusion imaging</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>TeRP</td>\n",
       "      <td>perfusion defects</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>drugs</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>TrCP</td>\n",
       "      <td>known allergies</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>metal plate</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>TrAP</td>\n",
       "      <td>gsw</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beth</td>\n",
       "      <td>record-105</td>\n",
       "      <td>creams</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>TrNAP</td>\n",
       "      <td>any incisions</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   file_name         from_text  from_line_number  from_begin_word_num  \\\n",
       "0   beth  record-105              cath                21                    0   \n",
       "1   beth  record-105  pefusion imaging                19                    6   \n",
       "2   beth  record-105             drugs                12                    8   \n",
       "3   beth  record-105       metal plate                26                    7   \n",
       "4   beth  record-105            creams               145                   14   \n",
       "\n",
       "   from_end_word_num relation_type            to_text  to_line_number  \\\n",
       "0                  0          TeRP      80% lm lesion              21   \n",
       "1                  7          TeRP  perfusion defects              19   \n",
       "2                  8          TrCP    known allergies              12   \n",
       "3                  8          TrAP                gsw              26   \n",
       "4                 14         TrNAP      any incisions             145   \n",
       "\n",
       "   to_begin_word_num  to_end_word_num  \n",
       "0                  6                8  \n",
       "1                 12               13  \n",
       "2                  5                6  \n",
       "3                 11               11  \n",
       "4                 20               21  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relation_type\n",
       "PIP      2203\n",
       "TeCP      504\n",
       "TeRP     3053\n",
       "TrAP     2617\n",
       "TrCP      526\n",
       "TrIP      203\n",
       "TrNAP     174\n",
       "TrWP      133\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_df.groupby(['relation_type']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 200.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 215.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 256/256 [00:01<00:00, 212.88it/s]\n"
     ]
    }
   ],
   "source": [
    "list_of_all_text = list()\n",
    "        \n",
    "for file in tqdm(os.listdir(beth_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(beth_file_path+r'\\txt', file)\n",
    "    file_name = file[0:-4]\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'beth'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)\n",
    "    \n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file[0:-4]\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'partners'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)\n",
    "    \n",
    "    \n",
    "for file in tqdm(os.listdir(test_data_texts_path)):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(test_data_texts_path, file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    new_text = dict()\n",
    "    new_text['source'] = 'test_data'\n",
    "    new_text['file_name'] = file_name\n",
    "    new_text['text'] = line\n",
    "    \n",
    "    list_of_all_text.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(list_of_all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list(text_df['file_name'])\n",
    "\n",
    "training_files = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/training_files.pkl\",\"rb\"))\n",
    "\n",
    "test_files = [entry for entry in all_files if entry not in training_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_map = dict()\n",
    "\n",
    "#Treatment improves medical problem\n",
    "sentence_map['TrIP'] = [\"{0} improves {1}.\", \"{1} can be treated by {0}.\"]\n",
    "\n",
    "#Treatment worsens medical problem\n",
    "sentence_map['TrWP'] = [\"{0} worsens {1}.\", \"{0} does not improve {1}.\", \"{0} does not cure {1}.\"]\n",
    "\n",
    "#Treatment causes medical problem\n",
    "sentence_map['TrCP'] = [\"{0} causes {1}.\", \"{0} results in {1}.\", \"{1} is a result of {0}.\"]\n",
    "\n",
    "#Treatment is administered for the medical problem\n",
    "sentence_map['TrAP'] = [\"{0} is prescribed for {1}.\", \"{0} is administered for {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['TrNAP'] = [\"{0} can not be prescribed due to {1}.\", \"{0} is not administered due to {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['TrNRP'] = [\"{0} and {1} has no relation.\"]\n",
    "\n",
    "#Test reveals medical problem\n",
    "sentence_map['TeRP'] = [\"{0} reveals {1}.\", \"{0} indicates {1}.\"]\n",
    "\n",
    "#Test conducted to investigate medical problem\n",
    "sentence_map['TeCP'] = [\"{0} is conducted to check {1}.\", \"{0} is performed to investigate {1}.\"]\n",
    "\n",
    "#Test and problem has no relation\n",
    "sentence_map['TeNRP'] = [\"{0} and {1} has no relation.\"]\n",
    "\n",
    "#Medical problem indicates medical problem\n",
    "sentence_map['PIP'] = [\"{1} can cause {0}.\", \"{0} is a result of {1}\"]\n",
    "\n",
    "#Medical problem indicates medical problem\n",
    "sentence_map['PNP'] = [\"{0} and {1} has no relation.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_map = dict()\n",
    "\n",
    "#Treatment improves medical problem\n",
    "sentence_map['TrIP'] = [\"{0} improves {1}.\"]\n",
    "\n",
    "#Treatment worsens medical problem\n",
    "sentence_map['TrWP'] = [\"{0} worsens {1}.\"]\n",
    "\n",
    "#Treatment causes medical problem\n",
    "sentence_map['TrCP'] = [\"{0} causes {1}.\"]\n",
    "\n",
    "#Treatment is administered for the medical problem\n",
    "sentence_map['TrAP'] = [\"{0} is administered for {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['TrNAP'] = [\"{0} is not administered due to {1}.\"]\n",
    "\n",
    "#Treatment is not administered because of medical problem\n",
    "sentence_map['NoREL'] = [\"{0} and {1} has relation.\"]\n",
    "\n",
    "#Test reveals medical problem\n",
    "sentence_map['TeRP'] = [\"{0} reveals {1}.\"]\n",
    "\n",
    "#Test conducted to investigate medical problem\n",
    "sentence_map['TeCP'] = [\"{0} is conducted to investigate {1}.\"]\n",
    "\n",
    "#Medical problem indicates medical problem\n",
    "sentence_map['PIP'] = [\"{1} indicates {0}.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_utility = BERT_utility()\n",
    "\n",
    "relation_encoding_list = list()\n",
    "relation_label_list = list()\n",
    "\n",
    "no_rel_count = 0\n",
    "rel_count = 0\n",
    "exist_rec_list = list()\n",
    "\n",
    "all_relations_dict = {\"treatment\":['TrIP','TrWP','TrCP','TrAP','TrNAP','NoREL'], \"test\":['TeRP','TeCP','NoREL'], \"problem\":['PIP','NoREL']}\n",
    "\n",
    "all_problems = concept_df[(concept_df['concept_type']=='problem') & (concept_df['file_name'].isin(training_files))]\n",
    "\n",
    "for index,row in all_problems.iterrows():\n",
    "    all_other_entities = concept_df[(concept_df['file_name']==row['file_name'])&(concept_df['source']==row['source'])&(concept_df['line_number']==row['line_number'])&(concept_df['begin_word_num']!=row['begin_word_num'])]\n",
    "    \n",
    "    for entity_index,entity_row in all_other_entities.iterrows():\n",
    "        \n",
    "        key_to_check = row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(entity_row[\"begin_word_num\"])+\"#\"+str(row[\"begin_word_num\"])\n",
    "        \n",
    "        if(key_to_check in exist_rec_list):\n",
    "            continue\n",
    "        else:\n",
    "            exist_rec_list.append(row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(row[\"begin_word_num\"])+\"#\"+str(entity_row[\"begin_word_num\"]))\n",
    "        \n",
    "        relation_df_record = relation_df[(relation_df['file_name']==row['file_name'])&(relation_df['source']==row['source'])&(relation_df['from_line_number']==row['line_number'])&(((relation_df['from_begin_word_num']==row['begin_word_num']) & (relation_df['to_begin_word_num']==entity_row['begin_word_num'])) |((relation_df['from_begin_word_num']==entity_row['begin_word_num']) & (relation_df['to_begin_word_num']==row['begin_word_num'])))]\n",
    "        \n",
    "        if(len(relation_df_record)==0):\n",
    "            relation = 'NoREL'\n",
    "        else:\n",
    "            relation = relation_df_record.iloc[0][\"relation_type\"]\n",
    "        \n",
    "        remaining_relations = [entry for entry in all_relations_dict[entity_row['concept_type']] if entry != relation]\n",
    "        \n",
    "        first_sentence = text_df[text_df['file_name']==row['file_name']].iloc[0]['text'].split(\"\\n\")[row['line_number']-1]\n",
    "        \n",
    "        '''\n",
    "        for entry in sentence_map[relation]:\n",
    "            second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "            \n",
    "            relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        if(relation != 'NoREL'):\n",
    "            for no_relation in remaining_relations:\n",
    "                for entry in sentence_map[no_relation]:\n",
    "                    second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "\n",
    "                    relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "                    relation_label_list.append(0)\n",
    "        '''\n",
    "        \n",
    "        if(relation=='NoREL'):\n",
    "            entry = sentence_map['NoREL'][0]\n",
    "            relation_label_list.append(0)\n",
    "        else:\n",
    "            entry = sentence_map['NoREL'][0]\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "        relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15396it [04:28, 57.37it/s]\n"
     ]
    }
   ],
   "source": [
    "nsp_utility = BERT_utility()\n",
    "\n",
    "relation_encoding_list = list()\n",
    "relation_label_list = list()\n",
    "relation_type_list = list()\n",
    "\n",
    "no_rel_count = 0\n",
    "rel_count = 0\n",
    "exist_rec_list = list()\n",
    "\n",
    "all_relations_dict = {\"treatment\":['TrIP','TrWP','TrCP','TrAP','TrNAP','NoREL'], \"test\":['TeRP','TeCP','NoREL'], \"problem\":['PIP','NoREL']}\n",
    "\n",
    "all_problems = concept_df[(concept_df['concept_type']=='problem') & (concept_df['file_name'].isin(training_files))]\n",
    "\n",
    "for index,row in tqdm(all_problems.iterrows()):\n",
    "    all_other_entities = concept_df[(concept_df['file_name']==row['file_name'])&(concept_df['source']==row['source'])&(concept_df['line_number']==row['line_number'])&(concept_df['begin_word_num']!=row['begin_word_num'])]\n",
    "    \n",
    "    for entity_index,entity_row in all_other_entities.iterrows():\n",
    "        \n",
    "        key_to_check = row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(entity_row[\"begin_word_num\"])+\"#\"+str(row[\"begin_word_num\"])\n",
    "        \n",
    "        if(key_to_check in exist_rec_list):\n",
    "            continue\n",
    "        else:\n",
    "            exist_rec_list.append(row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(row[\"begin_word_num\"])+\"#\"+str(entity_row[\"begin_word_num\"]))\n",
    "        \n",
    "        relation_df_record = relation_df[(relation_df['file_name']==row['file_name'])&(relation_df['source']==row['source'])&(relation_df['from_line_number']==row['line_number'])&(((relation_df['from_begin_word_num']==row['begin_word_num']) & (relation_df['to_begin_word_num']==entity_row['begin_word_num'])) |((relation_df['from_begin_word_num']==entity_row['begin_word_num']) & (relation_df['to_begin_word_num']==row['begin_word_num'])))]\n",
    "        \n",
    "        if(len(relation_df_record)==0):\n",
    "            relation = 'NoREL'\n",
    "        else:\n",
    "            relation = relation_df_record.iloc[0][\"relation_type\"]\n",
    "        \n",
    "        relation_label_list.append(relation_label_dict[relation])\n",
    "        \n",
    "        #remaining_relations = [entry for entry in all_relations_dict[entity_row['concept_type']] if entry != relation]\n",
    "        all_lines = text_df[text_df['file_name']==row['file_name']].iloc[0]['text'].split(\"\\n\")\n",
    "        \n",
    "        first_sentence = all_lines[row['line_number']-1]\n",
    "        \n",
    "        if(row['line_number']>1):\n",
    "            prev_sentence = all_lines[row['line_number']-2]\n",
    "        else:\n",
    "            prev_sentence = \"\"\n",
    "        \n",
    "        total_lines = len(all_lines)\n",
    "        \n",
    "        if(row['line_number']<total_lines):\n",
    "            next_sentence = all_lines[row['line_number']]\n",
    "        else:\n",
    "            next_sentence = \"\"\n",
    "        \n",
    "        first_sentence.replace(row['text'],\"@problem\")\n",
    "        \n",
    "        if(entity_row['concept_type']=='problem'):\n",
    "            relation_type_list.append(\"problem\")\n",
    "            first_sentence.replace(entity_row['text'],\"@related_problem\")\n",
    "        elif(entity_row['concept_type']=='test'):\n",
    "            relation_type_list.append(\"test\")\n",
    "            first_sentence.replace(entity_row['text'],\"@test\")\n",
    "        else:\n",
    "            relation_type_list.append(\"treatment\")\n",
    "            first_sentence.replace(entity_row['text'],\"@treatment\")\n",
    "        \n",
    "        \n",
    "        final_sentence = prev_sentence + \" . \"+ first_sentence + \". \" + next_sentence + \" . \"\n",
    "        \n",
    "        encodings = bert_tokenizer.encode(final_sentence, add_special_tokens=False)\n",
    "        \n",
    "        if(len(encodings)>511):\n",
    "            final_sentence = prev_sentence + \" . \"+ first_sentence\n",
    "        \n",
    "            encodings = bert_tokenizer.encode(final_sentence, add_special_tokens=False)\n",
    "            \n",
    "            if(len(encodings)>511):\n",
    "                final_sentence = first_sentence\n",
    "        \n",
    "                encodings = bert_tokenizer.encode(final_sentence, add_special_tokens=False)\n",
    "        \n",
    "        final_encoding = [101]\n",
    "        final_encoding.extend(encodings)\n",
    "        \n",
    "        '''\n",
    "        for entry in sentence_map[relation]:\n",
    "            second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "            \n",
    "            relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        if(relation != 'NoREL'):\n",
    "            for no_relation in remaining_relations:\n",
    "                for entry in sentence_map[no_relation]:\n",
    "                    second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "\n",
    "                    relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "                    relation_label_list.append(0)\n",
    "        \n",
    "        if(relation=='NoREL'):\n",
    "            entry = sentence_map['NoREL'][0]\n",
    "            relation_label_list.append(0)\n",
    "        else:\n",
    "            entry = sentence_map['NoREL'][0]\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "        '''\n",
    "        \n",
    "        relation_encoding_list.append(final_encoding)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23206"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relation_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2828"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in ptr_relation_label_list if x != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pte_relation_label_list = [relation_label_list[i] for i in range(len(relation_label_list)) if relation_type_list[i]=='test']\n",
    "pte_relation_encoding_list = [relation_encoding_list[i] for i in range(len(relation_encoding_list)) if relation_type_list[i]=='test']\n",
    "\n",
    "ptr_relation_label_list = [relation_label_list[i] for i in range(len(relation_label_list)) if relation_type_list[i]=='treatment']\n",
    "ptr_relation_encoding_list = [relation_encoding_list[i] for i in range(len(relation_encoding_list)) if relation_type_list[i]=='treatment']\n",
    "\n",
    "pp_relation_label_list = [relation_label_list[i] for i in range(len(relation_label_list)) if relation_type_list[i]=='problem']\n",
    "pp_relation_encoding_list = [relation_encoding_list[i] for i in range(len(relation_encoding_list)) if relation_type_list[i]=='problem']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pte_relation_label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/pte_relation_labels.pkl\",\"wb\"))\n",
    "pickle.dump(pte_relation_encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/pte_relation_encodings.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pte_relation_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ptr_relation_label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/ptr_relation_labels.pkl\",\"wb\"))\n",
    "pickle.dump(ptr_relation_encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/ptr_relation_encodings.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pp_relation_label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/pp_relation_labels.pkl\",\"wb\"))\n",
    "pickle.dump(pp_relation_encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/pp_relation_encodings.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_label_dict = {'NoREL':0, 'PIP':1, 'TeCP':1, 'TeRP':2, 'TrAP':1, 'TrCP':2, 'TrIP':3, 'TrNAP':4, 'TrWP':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(relation_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from BERT_config import bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_config['ncbi_base_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 101, 1045, 2066, 2009, 1012, 102]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [101]\n",
    "\n",
    "temp.extend(bert_tokenizer.encode(\"I like it.\"))\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NoREL', 15899),\n",
       " ('TeRP', 2352),\n",
       " ('TrAP', 1993),\n",
       " ('PIP', 1730),\n",
       " ('TrCP', 422),\n",
       " ('TeCP', 397),\n",
       " ('TrIP', 178),\n",
       " ('TrNAP', 133),\n",
       " ('TrWP', 102)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(rel_label_list).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_utility = BERT_utility()\n",
    "\n",
    "rel_label_list = list()\n",
    "\n",
    "exist_rec_list = list()\n",
    "\n",
    "all_relations_dict = {\"treatment\":['TrIP','TrWP','TrCP','TrAP','TrNAP','NoREL'], \"test\":['TeRP','TeCP','NoREL'], \"problem\":['PIP','NoREL']}\n",
    "\n",
    "all_problems = concept_df[(concept_df['concept_type']=='problem') & (concept_df['file_name'].isin(training_files))]\n",
    "\n",
    "for index,row in all_problems.iterrows():\n",
    "    all_other_entities = concept_df[(concept_df['file_name']==row['file_name'])&(concept_df['source']==row['source'])&(concept_df['line_number']==row['line_number'])&(concept_df['begin_word_num']!=row['begin_word_num'])]\n",
    "    \n",
    "    for entity_index,entity_row in all_other_entities.iterrows():\n",
    "        \n",
    "        key_to_check = row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(entity_row[\"begin_word_num\"])+\"#\"+str(row[\"begin_word_num\"])\n",
    "        \n",
    "        if(key_to_check in exist_rec_list):\n",
    "            continue\n",
    "        else:\n",
    "            exist_rec_list.append(row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(row[\"begin_word_num\"])+\"#\"+str(entity_row[\"begin_word_num\"]))\n",
    "        \n",
    "        relation_df_record = relation_df[(relation_df['file_name']==row['file_name'])&(relation_df['source']==row['source'])&(relation_df['from_line_number']==row['line_number'])&(((relation_df['from_begin_word_num']==row['begin_word_num']) & (relation_df['to_begin_word_num']==entity_row['begin_word_num'])) |((relation_df['from_begin_word_num']==entity_row['begin_word_num']) & (relation_df['to_begin_word_num']==row['begin_word_num'])))]\n",
    "        \n",
    "        if(len(relation_df_record)==0):\n",
    "            relation = 'NoREL'\n",
    "        else:\n",
    "            relation = relation_df_record.iloc[0][\"relation_type\"]\n",
    "        \n",
    "        rel_label_list.append(relation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list = [entry[0] for entry in relation_encoding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list = [list(entry.detach().numpy()) for entry in relation_encoding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list = [list(np.array(entry)) for entry in relation_encoding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list = list(relation_encoding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ entry for entry in relation_label_list if entry==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"blank\":0,\"problem\":1,\"test\":2,\"treatment\":3}\n",
    "\n",
    "encoding_list = list()\n",
    "label_list = list()\n",
    "utility = BERT_utility(get_encodings=True, get_embeddings=False)\n",
    "\n",
    "for file_name in training_files:\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    encoding = utility.encoding_list\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(training_files,open(\"C:/Users/itsma/Documents/CS 6120 Project/training_files.pkl\",\"wb\"))\n",
    "pickle.dump(test_files,open(\"C:/Users/itsma/Documents/CS 6120 Project/test_files.pkl\",\"wb\"))\n",
    "pickle.dump(encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/input_ids.pkl\",\"wb\"))\n",
    "pickle.dump(label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/label.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(concept_df,open(\"C:/Users/itsma/Documents/CS 6120 Project/concept_df.pkl\",\"wb\"))\n",
    "pickle.dump(relation_df,open(\"C:/Users/itsma/Documents/CS 6120 Project/relation_df.pkl\",\"wb\"))\n",
    "pickle.dump(text_df,open(\"C:/Users/itsma/Documents/CS 6120 Project/text_df.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/input_ids.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(relation_encoding_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/relation_encodings.pkl\",\"wb\"))\n",
    "pickle.dump(relation_label_list,open(\"C:/Users/itsma/Documents/CS 6120 Project/relation_labels.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list = list()\n",
    "label_list = list()\n",
    "utility = BERT_utility()\n",
    "\n",
    "for file in tqdm(os.listdir(beth_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(beth_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='beth')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)\n",
    "\n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        \n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)\n",
    "    \n",
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    positions = create_pos_dict(file_concepts)\n",
    "    \n",
    "    prior_sentence_index = -1\n",
    "    \n",
    "    word_list, encoding = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for i in range(len(all_lines)):\n",
    "        labels = [0] * len(encoding[i])\n",
    "        \n",
    "        fil_word_list = [word for word in word_list if word[\"sentence_index\"]==i+1] \n",
    "        for entry in fil_word_list:\n",
    "            key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "            if(key in positions):\n",
    "                for token_position in entry[\"bert_token_positions\"]:\n",
    "                    labels[token_position] = class_map[positions[key]]\n",
    "        label_list.append(labels)\n",
    "    \n",
    "    encoding_list.extend(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'patient',\n",
       " 'recorded',\n",
       " 'as',\n",
       " 'having',\n",
       " 'known',\n",
       " 'all',\n",
       " '##er',\n",
       " '##gies',\n",
       " 'in',\n",
       " 'response',\n",
       " 'to',\n",
       " 'drug',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"Patient recorded as having known allergies in response to drug.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer.encode(\"Hello, my dog is cute\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1,0,2,0,3,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'mri',\n",
       " 'scan',\n",
       " 'showed',\n",
       " 'white',\n",
       " 'spots',\n",
       " 'in',\n",
       " 'brain',\n",
       " 'and',\n",
       " 'he',\n",
       " 'was',\n",
       " 'administered',\n",
       " 'ben',\n",
       " '##fo',\n",
       " '##tia',\n",
       " '##mine',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"MRI Scan showed white spots in brain and he was administered Benfotiamine.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "        \n",
    "outputs = model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[0][0][5].data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BERT_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(BERT_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility = BERT_utility()\n",
    "#word_list = utility.process_string_finetune(line,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 340/340 [24:29<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "all_words_list = list()\n",
    "\n",
    "utility = BERT_utility(get_embeddings=True,get_encodings=False,use_finetuned_model=True)\n",
    "\n",
    "for file_name in tqdm(training_files):\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[assertion_df['file_name']==file_name]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 86/86 [07:20<00:00,  5.13s/it]\n"
     ]
    }
   ],
   "source": [
    "all_words_list_test = list()\n",
    "\n",
    "utility = BERT_utility(get_embeddings=True,get_encodings=False,use_finetuned_model=True)\n",
    "\n",
    "for file_name in tqdm(test_files):\n",
    "    text = text_df[text_df['file_name']==file_name].iloc[0]['text']\n",
    "    all_lines = text.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = concept_df[concept_df['file_name']==file_name]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[assertion_df['file_name']==file_name]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(text,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list_test.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'keyword_vector', 'sentence_index', 'word_index', 'concept',\n",
       "       'assertion'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = word_df[word_df['concept']=='problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assertion = np.vstack(list(filt_df[\"keyword_vector\"]))                                \n",
    "y_assertion =list(filt_df[\"assertion\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept = np.vstack(list(word_df[\"keyword_vector\"]))                                \n",
    "y_concept =list(word_df[\"concept\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_concept = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9954334855504816"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_concept.score(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_assertion = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9529860570077641"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_assertion.score(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df_test = pd.DataFrame(all_words_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept_test = np.vstack(list(word_df_test[\"keyword_vector\"]))                                \n",
    "y_concept_test =list(word_df_test[\"concept\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.971349428031664"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_concept.score(X_concept_test, y_concept_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_score(y_concept_test,y_predict_concept,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93780271, 0.9370384 , 0.93038412])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_concept = clf_concept.predict(X_concept_test)\n",
    "f1_score(y_concept_test,y_predict_concept,average=None, labels = ['problem','test','treatment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93354974 0.9461223  0.93713945]\n",
      "[0.94209461 0.92812729 0.92372549]\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_concept_test,y_predict_concept,average=None, labels = ['problem','test','treatment']))\n",
    "print(recall_score(y_concept_test,y_predict_concept,average=None, labels = ['problem','test','treatment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df_test = word_df_test[word_df_test['concept']=='problem']\n",
    "\n",
    "X_assertion_test = np.vstack(list(filt_df_test[\"keyword_vector\"]))                                \n",
    "y_assertion_test =list(filt_df_test[\"assertion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9252336448598131"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_assertion.score(X_assertion_test, y_assertion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_assertion = clf_assertion.predict(X_assertion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8249497 , 0.26666667, 0.65      , 0.61139896, 0.95643478,\n",
       "       0.91396794])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_assertion_test, y_predict_assertion, average=None,labels=labels_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89519651 0.61538462 0.79591837 0.70574163 0.93808239 0.93071286]\n",
      "[0.76492537 0.17021277 0.54929577 0.5393053  0.97551957 0.89781491]\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_assertion_test, y_predict_assertion, average=None,labels=labels_assertion))\n",
    "print(recall_score(y_assertion_test, y_predict_assertion, average=None,labels=labels_assertion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 205,    0,    0,    1,   44,   18],\n",
       "       [   2,   16,    0,    0,   76,    0],\n",
       "       [   0,    0,   39,    0,   26,    6],\n",
       "       [   8,    0,    3,  295,  225,   16],\n",
       "       [  13,    9,    1,  105, 7651,   64],\n",
       "       [   1,    1,    6,   17,  134, 1397]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_assertion = ['hypothetical',\n",
    " 'conditional',\n",
    " 'associated_with_someone_else',\n",
    " 'possible',\n",
    " 'present',\n",
    " 'absent']\n",
    "\n",
    "confusion_matrix(y_assertion_test,y_predict_assertion,labels_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(os.listdir(partners_file_path+r'\\txt')):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(partners_file_path+r'\\txt', file)\n",
    "    file_name = file.strip(\".txt\")\n",
    "    oFile = open(file_path, 'r')\n",
    "    line = oFile.read()\n",
    "    \n",
    "    all_lines = line.split(\"\\n\")\n",
    "    file_concepts = concept_df[(concept_df['file_name']==file_name)&(concept_df['source']=='partners')]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = assertion_df[(assertion_df['file_name']==file_name)&(assertion_df['source']=='partners')]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_df[words_df['concept']=='problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(words_df['concept']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"a\":\"1\", \"b\":\"2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concept = np.vstack(list(words_df[\"keyword_vector\"]))                                \n",
    "y_concept = words_df[\"concept\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assertion = np.vstack(list(filt_df[\"keyword_vector\"]))                                \n",
    "y_assertion =list(filt_df[\"assertion\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = words_df[words_df['assertion']!='blank'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_concept = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_concept, y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(y_assertion)):\n",
    "    \n",
    "    if(y_assertion[i] not in ['absent', 'associated_with_someone_else', 'conditional', 'hypothetical', 'possible', 'present']):\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_concept.score(X_concept,y_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(X_assertion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion = LogisticRegression(random_state=0,solver=\"lbfgs\",max_iter=1000).fit(X_assertion, y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion.score(X_assertion,y_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data' \n",
    "\n",
    "list_of_all_test_concepts = list()\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\concepts'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\concepts', file)\n",
    "    list_of_all_test_concepts.extend(get_concepts_from_file(file_path,file.strip(\".con\"),'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concept_df = pd.DataFrame(list_of_all_test_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concept_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = os.path.dirname(os.getcwd()) + r'\\Data\\reference_standard_for_test_data' \n",
    "\n",
    "list_of_all_test_assertions = list()\n",
    "\n",
    "for file in os.listdir(test_data_file_path+r'\\ast'):\n",
    "    file_path = os.path.join(test_data_file_path+r'\\ast', file)\n",
    "    list_of_all_test_assertions.extend(get_assertions_from_file(file_path,file[0:-4],'test_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assertion_df = pd.DataFrame(list_of_all_test_assertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_assertion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list_test = list()\n",
    "test_data_texts_path =  os.path.dirname(os.getcwd()) + r'\\Data\\test_data'\n",
    "for file in tqdm(os.listdir(test_data_texts_path)):\n",
    "    if(not file.endswith(\".txt\")):\n",
    "        continue\n",
    "    file_path = os.path.join(test_data_texts_path, file)\n",
    "    \n",
    "    file_name = file.strip(\".txt\")\n",
    "    \n",
    "    oFile = open(file_path, 'r')\n",
    "    \n",
    "    line = oFile.read()\n",
    "    \n",
    "    all_lines = line.split(\"\\n\")\n",
    "    \n",
    "    file_concepts = test_concept_df[(test_concept_df['file_name']==file_name)&(test_concept_df['source']=='test_data')]\n",
    "    concept_positions = create_pos_dict_concept(file_concepts)\n",
    "    \n",
    "    file_assertions = test_assertion_df[(test_assertion_df['file_name']==file_name)&(test_assertion_df['source']=='test_data')]\n",
    "    assertion_positions = create_pos_dict_assertion(file_assertions)\n",
    "    \n",
    "    word_list = utility.process_string_finetune(line,0)\n",
    "    \n",
    "    for entry in word_list:\n",
    "        key = str(entry['sentence_index'])+ \":\" +str(entry['word_index'])\n",
    "        \n",
    "        if(key in concept_positions):\n",
    "            entry.update({\"concept\":concept_positions[key]})\n",
    "            \n",
    "            if(concept_positions[key]=='problem'):\n",
    "                if(key in assertion_positions):\n",
    "                    entry.update({\"assertion\":assertion_positions[key]})\n",
    "                else:\n",
    "                    entry.update({\"assertion\":\"blank\"})\n",
    "            else:\n",
    "                entry.update({\"assertion\":\"blank\"})\n",
    "        else:\n",
    "            entry.update({\"concept\":\"blank\"})\n",
    "            entry.update({\"assertion\":\"blank\"})\n",
    "    \n",
    "    all_words_list_test.extend(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_df = pd.DataFrame(all_words_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filt_df = test_word_df[test_word_df['assertion']!='blank'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_concept = np.vstack(list(test_word_df[\"keyword_vector\"]))                                \n",
    "y_test_concept = test_word_df[\"concept\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_assertion = np.vstack(list(test_filt_df[\"keyword_vector\"]))                                \n",
    "y_test_assertion = test_filt_df[\"assertion\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_assertion.score(X_test_assertion,y_test_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_assertion = clf_assertion.predict(X_test_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test_assertion, y_predict_assertion, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_assertion = ['hypothetical',\n",
    " 'conditional',\n",
    " 'associated_with_someone_else',\n",
    " 'possible',\n",
    " 'present',\n",
    " 'absent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['blank', 'problem', 'test', 'treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_assertion,y_predict_assertion,labels_assertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{1} can be treated {0}\".format(\"Tylenol\",\"headache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_predict, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 2381, 1997, 2556, 7355, 1024, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/CS6120/Model/finetuned_model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer.encode(\"Hello, my dog is cute\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "        \n",
    "outputs = finetuned_model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = [entry for entry in label_list if 3 in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/CS6120/Model/finetuned_model.pkl\",\"rb\"))\n",
    "bert_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = bert_tokenizer.encode(\"i love my country\",add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from BERT_config import bert_config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_config['ncbi_base_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(encodings).long().unsqueeze(0)\n",
    "\n",
    "outputs = bert_model(input_ids,token_type_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[1][12][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_model = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/CS6120/Model/relation_finetuned.pkl\",\"rb\"))\n",
    "relation_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_model = relation_model.to('cuda')\n",
    "relation_model = relation_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_utility = BERT_utility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ip  = torch.tensor(pad_sequences([nsp_utility.get_embeddings_for_nsp(\"i like\", \"you like\")],maxlen=512, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ip = temp_ip.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_op = relation_model(temp_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "c = torch.cuda.memory_cached(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = c-a  # free inside cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_model_cp = relation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del relation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_op = relation_model_cp(temp_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_1 = temp_op[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_op[1][12][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_utility = BERT_utility()\n",
    "\n",
    "relation_encoding_list = list()\n",
    "relation_embedding_list = list()\n",
    "relation_label_list = list()\n",
    "\n",
    "no_rel_count = 0\n",
    "rel_count = 0\n",
    "exist_rec_list = list()\n",
    "\n",
    "all_relations_dict = {\"treatment\":['TrIP','TrWP','TrCP','TrAP',\"TrNRP\"], \"test\":['TeRP','TeCP','TeNRP'], \"problem\":['PIP','PNP']}\n",
    "no_relations_dict = {\"treatment\":\"TrNRP\", \"test\":\"TeNRP\", \"problem\":\"PNP\"}\n",
    "\n",
    "all_problems = concept_df[(concept_df['concept_type']=='problem') & (concept_df['file_name'].isin(test_files))]\n",
    "\n",
    "for index,row in tqdm(all_problems.iterrows()):\n",
    "    all_other_entities = concept_df[(concept_df['file_name']==row['file_name'])&(concept_df['source']==row['source'])&(concept_df['line_number']==row['line_number'])&(concept_df['begin_word_num']!=row['begin_word_num'])]\n",
    "    \n",
    "    for entity_index,entity_row in all_other_entities.iterrows():\n",
    "        \n",
    "        key_to_check = row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(entity_row[\"begin_word_num\"])+\"#\"+str(row[\"begin_word_num\"])\n",
    "        \n",
    "        if(key_to_check in exist_rec_list):\n",
    "            continue\n",
    "        else:\n",
    "            exist_rec_list.append(row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(row[\"begin_word_num\"])+\"#\"+str(entity_row[\"begin_word_num\"]))\n",
    "        \n",
    "        relation_df_record = relation_df[(relation_df['file_name']==row['file_name'])&(relation_df['source']==row['source'])&(relation_df['from_line_number']==row['line_number'])&(((relation_df['from_begin_word_num']==row['begin_word_num']) & (relation_df['to_begin_word_num']==entity_row['begin_word_num'])) |((relation_df['from_begin_word_num']==entity_row['begin_word_num']) & (relation_df['to_begin_word_num']==row['begin_word_num'])))]\n",
    "        \n",
    "        if(len(relation_df_record)==0):\n",
    "            relation = no_relations_dict[entity_row[\"concept_type\"]]\n",
    "        else:\n",
    "            relation = relation_df_record.iloc[0][\"relation_type\"]\n",
    "        \n",
    "        remaining_relations = [entry for entry in all_relations_dict[entity_row['concept_type']] if entry != relation]\n",
    "        \n",
    "        first_sentence = text_df[text_df['file_name']==row['file_name']].iloc[0]['text'].split(\"\\n\")[row['line_number']-1]\n",
    "        \n",
    "        for entry in sentence_map[relation]:\n",
    "            second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "            \n",
    "            relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        for no_relation in remaining_relations:\n",
    "            for entry in sentence_map[no_relation]:\n",
    "                second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "\n",
    "                relation_encoding_list.append(nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence))\n",
    "                relation_label_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concept_df[(concept_df['concept_type']=='problem') & (concept_df['file_name'].isin(training_files))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_model = relation_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_utility = BERT_utility()\n",
    "\n",
    "relation_encoding_list = list()\n",
    "relation_embedding_list = list()\n",
    "relation_label_list = list()\n",
    "\n",
    "no_rel_count = 0\n",
    "rel_count = 0\n",
    "exist_rec_list = list()\n",
    "\n",
    "all_relations_dict = {\"treatment\":['TrIP','TrWP','TrCP','TrAP',\"TrNRP\"], \"test\":['TeRP','TeCP','TeNRP'], \"problem\":['PIP','PNP']}\n",
    "no_relations_dict = {\"treatment\":\"TrNRP\", \"test\":\"TeNRP\", \"problem\":\"PNP\"}\n",
    "\n",
    "all_problems = concept_df[(concept_df['concept_type']=='problem') & (concept_df['file_name'].isin(training_files))]\n",
    "\n",
    "for index,row in tqdm(all_problems.iterrows()):\n",
    "    all_other_entities = concept_df[(concept_df['file_name']==row['file_name'])&(concept_df['source']==row['source'])&(concept_df['line_number']==row['line_number'])&(concept_df['begin_word_num']!=row['begin_word_num'])]\n",
    "    \n",
    "    for entity_index,entity_row in all_other_entities.iterrows():\n",
    "        \n",
    "        key_to_check = row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(entity_row[\"begin_word_num\"])+\"#\"+str(row[\"begin_word_num\"])\n",
    "        \n",
    "        if(key_to_check in exist_rec_list):\n",
    "            continue\n",
    "        else:\n",
    "            exist_rec_list.append(row[\"source\"]+\"#\"+row[\"file_name\"]+\"#\"+str(row[\"line_number\"])+\"#\"+str(row[\"begin_word_num\"])+\"#\"+str(entity_row[\"begin_word_num\"]))\n",
    "        \n",
    "        relation_df_record = relation_df[(relation_df['file_name']==row['file_name'])&(relation_df['source']==row['source'])&(relation_df['from_line_number']==row['line_number'])&(((relation_df['from_begin_word_num']==row['begin_word_num']) & (relation_df['to_begin_word_num']==entity_row['begin_word_num'])) |((relation_df['from_begin_word_num']==entity_row['begin_word_num']) & (relation_df['to_begin_word_num']==row['begin_word_num'])))]\n",
    "        \n",
    "        if(len(relation_df_record)==0):\n",
    "            relation = \"NoREL\"\n",
    "        else:\n",
    "            relation = relation_df_record.iloc[0][\"relation_type\"]\n",
    "        \n",
    "        \n",
    "        #remaining_relations = [entry for entry in all_relations_dict[entity_row['concept_type']] if entry != relation]\n",
    "        \n",
    "        first_sentence = text_df[text_df['file_name']==row['file_name']].iloc[0]['text'].split(\"\\n\")[row['line_number']-1]\n",
    "        \n",
    "        '''\n",
    "        for entry in sentence_map[relation]:\n",
    "            second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "            \n",
    "            encoding = nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence)\n",
    "            \n",
    "            input_ids = pad_sequences([encoding],maxlen=512, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "            \n",
    "            input_tensor = torch.tensor(input_ids).long()\n",
    "            \n",
    "            input_tensor = input_tensor.to('cuda')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedding = relation_model(input_tensor)\n",
    "            \n",
    "            input_tensor = input_tensor.to('cpu')\n",
    "            \n",
    "            relation_encoding_list.append(encoding)\n",
    "            relation_embedding_list.append(embedding[1][12][0][0])\n",
    "            relation_label_list.append(1)\n",
    "        \n",
    "        for no_relation in remaining_relations:\n",
    "            for entry in sentence_map[no_relation]:\n",
    "                second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "                \n",
    "                encoding = nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence)\n",
    "            \n",
    "                input_ids = pad_sequences([encoding],maxlen=512, dtype=\"long\", \n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "                input_tensor = torch.tensor(input_ids).long()\n",
    "\n",
    "                input_tensor = input_tensor.to('cuda')\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    embedding = relation_model(input_tensor, token_type_ids=None)\n",
    "                    \n",
    "                input_tensor = input_tensor.to('cpu')\n",
    "                \n",
    "                relation_encoding_list.append(encoding)\n",
    "                relation_embedding_list.append(embedding[1][12][0][0])\n",
    "                relation_label_list.append(0)\n",
    "        '''\n",
    "        \n",
    "        if()\n",
    "        second_sentence = entry.format(entity_row['text'],row['text'])\n",
    "                \n",
    "        encoding = nsp_utility.get_embeddings_for_nsp(first_sentence,second_sentence)\n",
    "\n",
    "        input_ids = pad_sequences([encoding],maxlen=512, dtype=\"long\", \n",
    "                      value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "        input_tensor = torch.tensor(input_ids).long()\n",
    "\n",
    "        input_tensor = input_tensor.to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = relation_model(input_tensor, token_type_ids=None)\n",
    "\n",
    "        input_tensor = input_tensor.to('cpu')\n",
    "\n",
    "        relation_encoding_list.append(encoding)\n",
    "        relation_embedding_list.append(embedding[1][12][0][0])\n",
    "        relation_label_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relation_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encoding_list_test = torch.tensor(pad_sequences(relation_encoding_list[0:100],maxlen=512, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_model.cpu()\n",
    "outputs = relation_model(relation_encoding_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_op = outputs[0].detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np_op).index(max(np_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = list()\n",
    "for i in tqdm(false_index[0:100]):\n",
    "    relation_encoding_test = torch.tensor(pad_sequences([relation_encoding_list[i]],maxlen=512, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")).long()\n",
    "    \n",
    "    outputs = relation_model(relation_encoding_test)\n",
    "    np_op = outputs[0].detach().numpy()[0]\n",
    "    predicted.append(list(np_op).index(max(np_op)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(true_labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_index = [index for index in range(len(relation_label_list)) if relation_label_list[index]==1]\n",
    "false_index = [index for index in range(len(relation_label_list)) if relation_label_list[index]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [relation_label_list[i] for i in false_index[0:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rel_data = pickle.load(open(\"C:/Users/itsma/Documents/CS 6120 Project/rel_data_df.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'axes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-1afa15e997cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_rel_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0minfo\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mhere\u001b[0m \u001b[0mwe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m         \"\"\"\n\u001b[1;32m--> 981\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5173\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5174\u001b[0m         ):\n\u001b[1;32m-> 5175\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'axes'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rel_vec = np.vstack(list(test_word_df[\"keyword_vector\"]))                                \n",
    "y_test_concept = test_word_df[\"concept\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondab189137550194d64ad595e9dd8ef8b75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
